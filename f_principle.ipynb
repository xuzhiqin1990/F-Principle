{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ml_collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"alpha\": 0,\n",
    "    \"x_start\": -5,\n",
    "    \"x_end\": 5,\n",
    "    \"train_size\": 101,\n",
    "    \"test_size\": 51,\n",
    "    \"layer_list\": [200, 200, 200, 100],\n",
    "    \"seed\": 0,\n",
    "    \"ActFun\": \"tanh\",\n",
    "    \"dims_input\": 1,\n",
    "    \"dims_output\": 1,\n",
    "    \"lossfunc\": \"MSE\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"epochs\": 300,\n",
    "    \"batch_size\": 50,\n",
    "    \"lr\": 2e-4,\n",
    "    \"lr_decayrate\": 0,\n",
    "    \"lr_decaystep\": 2000,\n",
    "    \"isFFT\": True,\n",
    "    \"result_dir\": 'results'\n",
    "}\n",
    "config = ml_collections.ConfigDict(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Function\n",
    "Design a target function by discretizing a smooth function $f_{0}(x)$ by\n",
    "$$f(x) = \\alpha * Round(f_{0}(x)/\\alpha)$$\n",
    "In the case, we use $f_{0}(x) = \\sin(x) + 2\\sin(x) + 3\\sin(5x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func0(xx):\n",
    "    y_sin = np.sin(xx)+2*np.sin(3*xx)+3*np.sin(5*xx)\n",
    "    return y_sin\n",
    "\n",
    "\n",
    "def func_to_approx(xx, alpha):\n",
    "    y_sin = func0(xx)\n",
    "    if alpha == 0:\n",
    "        return y_sin\n",
    "    out_y = np.round(y_sin/alpha)\n",
    "    out_y2 = out_y * alpha\n",
    "    return out_y2\n",
    "\n",
    "\n",
    "def get_data(config):\n",
    "    train_input = np.reshape(np.linspace(config.x_start, config.x_end,\n",
    "                             num=config.train_size, endpoint=True), [config.train_size, 1]).astype(np.float32)\n",
    "\n",
    "    y_train = func_to_approx(train_input, config.alpha)\n",
    "\n",
    "    return (train_input, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-222e5c90fcf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "config.alpha = 0\n",
    "train_data = get_data(config)\n",
    "x, y = train_data\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fft(data, freq_len=40, x_input=np.zeros(10), kk=0, min_f=0, max_f=np.pi/3, isnorm=1):\n",
    "    second_diff_input = np.mean(np.diff(np.diff(np.squeeze(x_input))))\n",
    "    if abs(second_diff_input) < 1e-10:\n",
    "        datat = np.squeeze(data)\n",
    "        datat_fft = np.fft.fft(datat)\n",
    "        ind2 = range(freq_len)\n",
    "        fft_coe = datat_fft[ind2]\n",
    "        if isnorm == 1:\n",
    "            return_fft = np.absolute(fft_coe)\n",
    "        else:\n",
    "            return_fft = fft_coe\n",
    "    else:\n",
    "        return_fft = get_ft_multi(\n",
    "            x_input, data, kk=kk, freq_len=freq_len, min_f=min_f, max_f=max_f, isnorm=isnorm)\n",
    "    return return_fft\n",
    "\n",
    "\n",
    "def get_ft_multi(x_input, data, kk=0, freq_len=100, min_f=0, max_f=np.pi/3, isnorm=1):\n",
    "    n = x_input.shape[1]\n",
    "    if np.max(abs(kk)) == 0:\n",
    "        k = np.linspace(min_f, max_f, num=freq_len, endpoint=True)\n",
    "        kk = np.matmul(np.ones([n, 1]), np.reshape(k, [1, -1]))\n",
    "    tmp = np.matmul(np.transpose(data), np.exp(-1J * (np.matmul(x_input, kk))))\n",
    "    if isnorm == 1:\n",
    "        return_fft = np.absolute(tmp)\n",
    "    else:\n",
    "        return_fft = tmp\n",
    "    return np.squeeze(return_fft)\n",
    "\n",
    "\n",
    "def SelectPeakIndex(FFT_Data, endpoint=True):\n",
    "    D1 = FFT_Data[1:-1]-FFT_Data[0:-2]\n",
    "    D2 = FFT_Data[1:-1]-FFT_Data[2:]\n",
    "    D3 = np.logical_and(D1 > 0, D2 > 0)\n",
    "    tmp = np.where(D3 == True)\n",
    "    sel_ind = tmp[0]+1\n",
    "    if endpoint:\n",
    "        if FFT_Data[0]-FFT_Data[1] > 0:\n",
    "            sel_ind = np.concatenate([[0], sel_ind])\n",
    "        if FFT_Data[-1]-FFT_Data[-2] > 0:\n",
    "            Last_ind = len(FFT_Data)-1\n",
    "            sel_ind = np.concatenate([sel_ind, [Last_ind]])\n",
    "    return sel_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        # self.compiled_loss(y, y_pred)\n",
    "        # self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return_metrics = {}\n",
    "        # for metric in self.metrics:\n",
    "        #     return_metrics[metric.name] = metric.result()\n",
    "        return_metrics['y_pred'] = y_pred\n",
    "\n",
    "        return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.keras.Input(shape=(config.dims_input, ))\n",
    "x = input\n",
    "for layer in config.layer_list:\n",
    "    x = tf.keras.layers.Dense(layer, activation=config.ActFun)(x)\n",
    "output = tf.keras.layers.Dense(config.dims_output)(x)\n",
    "\n",
    "model = CustomModel(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 3.0087 - val_loss: 2.8662 - val_y_pred: -0.0058\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8755 - val_loss: 2.9393 - val_y_pred: -0.0060\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.9505 - val_loss: 2.8943 - val_y_pred: -0.0061\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9045 - val_loss: 2.8373 - val_y_pred: -0.0059\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8462 - val_loss: 2.8426 - val_y_pred: -0.0054\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8510 - val_loss: 2.8723 - val_y_pred: -0.0044\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8804 - val_loss: 2.8633 - val_y_pred: -0.0029\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8716 - val_loss: 2.8311 - val_y_pred: -0.0012\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8397 - val_loss: 2.8163 - val_y_pred: 6.7215e-04\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8256 - val_loss: 2.8267 - val_y_pred: 0.0027\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8370 - val_loss: 2.8365 - val_y_pred: 0.0046\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8480 - val_loss: 2.8278 - val_y_pred: 0.0064\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.8401 - val_loss: 2.8088 - val_y_pred: 0.0083\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8221 - val_loss: 2.7990 - val_y_pred: 0.0097\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8131 - val_loss: 2.8041 - val_y_pred: 0.0111\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8186 - val_loss: 2.8113 - val_y_pred: 0.0122\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8257 - val_loss: 2.8080 - val_y_pred: 0.0129\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8219 - val_loss: 2.7974 - val_y_pred: 0.0135\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8106 - val_loss: 2.7917 - val_y_pred: 0.0136\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8041 - val_loss: 2.7942 - val_y_pred: 0.0135\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.8061 - val_loss: 2.7981 - val_y_pred: 0.0130\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8095 - val_loss: 2.7960 - val_y_pred: 0.0124\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8069 - val_loss: 2.7886 - val_y_pred: 0.0116\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7993 - val_loss: 2.7835 - val_y_pred: 0.0105\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7941 - val_loss: 2.7836 - val_y_pred: 0.0096\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7943 - val_loss: 2.7847 - val_y_pred: 0.0085\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7957 - val_loss: 2.7817 - val_y_pred: 0.0074\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7929 - val_loss: 2.7757 - val_y_pred: 0.0065\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7873 - val_loss: 2.7720 - val_y_pred: 0.0056\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7839 - val_loss: 2.7717 - val_y_pred: 0.0046\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7838 - val_loss: 2.7712 - val_y_pred: 0.0039\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7836 - val_loss: 2.7680 - val_y_pred: 0.0033\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7806 - val_loss: 2.7637 - val_y_pred: 0.0026\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7765 - val_loss: 2.7611 - val_y_pred: 0.0022\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7741 - val_loss: 2.7607 - val_y_pred: 0.0018\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7738 - val_loss: 2.7598 - val_y_pred: 0.0013\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7727 - val_loss: 2.7567 - val_y_pred: 0.0011\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7697 - val_loss: 2.7537 - val_y_pred: 7.1924e-04\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7665 - val_loss: 2.7524 - val_y_pred: 3.3095e-04\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7650 - val_loss: 2.7516 - val_y_pred: -2.7745e-06\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.7641 - val_loss: 2.7496 - val_y_pred: -3.0000e-04\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7621 - val_loss: 2.7467 - val_y_pred: -6.4718e-04\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7591 - val_loss: 2.7446 - val_y_pred: -9.4871e-04\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7569 - val_loss: 2.7435 - val_y_pred: -0.0012\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7556 - val_loss: 2.7416 - val_y_pred: -0.0014\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7539 - val_loss: 2.7393 - val_y_pred: -0.0016\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7514 - val_loss: 2.7367 - val_y_pred: -0.0016\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7489 - val_loss: 2.7350 - val_y_pred: -0.0016\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7473 - val_loss: 2.7332 - val_y_pred: -0.0015\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.7457 - val_loss: 2.7309 - val_y_pred: -0.0013\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7435 - val_loss: 2.7285 - val_y_pred: -0.0011\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7412 - val_loss: 2.7265 - val_y_pred: -8.7388e-04\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7393 - val_loss: 2.7246 - val_y_pred: -5.2829e-04\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7376 - val_loss: 2.7226 - val_y_pred: -2.1975e-04\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7356 - val_loss: 2.7203 - val_y_pred: 9.1527e-05\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7333 - val_loss: 2.7183 - val_y_pred: 3.5561e-04\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7313 - val_loss: 2.7166 - val_y_pred: 4.9078e-04\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7295 - val_loss: 2.7146 - val_y_pred: 5.8777e-04\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7274 - val_loss: 2.7124 - val_y_pred: 6.5425e-04\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7254 - val_loss: 2.7102 - val_y_pred: 6.9946e-04\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7233 - val_loss: 2.7083 - val_y_pred: 6.5230e-04\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7214 - val_loss: 2.7061 - val_y_pred: 6.4937e-04\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7193 - val_loss: 2.7039 - val_y_pred: 5.4628e-04\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7171 - val_loss: 2.7019 - val_y_pred: 4.8159e-04\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7152 - val_loss: 2.7000 - val_y_pred: 3.2132e-04\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7131 - val_loss: 2.6980 - val_y_pred: 2.0744e-04\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7111 - val_loss: 2.6959 - val_y_pred: 1.0903e-04\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7090 - val_loss: 2.6938 - val_y_pred: 1.3362e-04\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7070 - val_loss: 2.6918 - val_y_pred: 1.2585e-04\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7050 - val_loss: 2.6897 - val_y_pred: 7.3913e-05\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7030 - val_loss: 2.6876 - val_y_pred: 1.8098e-04\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7009 - val_loss: 2.6856 - val_y_pred: 7.5874e-05\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6988 - val_loss: 2.6833 - val_y_pred: 6.7472e-05\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6968 - val_loss: 2.6812 - val_y_pred: 9.8570e-06\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6947 - val_loss: 2.6790 - val_y_pred: 1.4616e-04\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6925 - val_loss: 2.6767 - val_y_pred: 4.0835e-06\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6904 - val_loss: 2.6747 - val_y_pred: -6.7832e-06\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6884 - val_loss: 2.6724 - val_y_pred: -1.5855e-05\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6861 - val_loss: 2.6703 - val_y_pred: 9.4223e-05\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6840 - val_loss: 2.6682 - val_y_pred: 2.8261e-05\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6820 - val_loss: 2.6660 - val_y_pred: 2.6007e-05\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6798 - val_loss: 2.6637 - val_y_pred: 9.7455e-05\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6775 - val_loss: 2.6615 - val_y_pred: 8.3119e-05\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6754 - val_loss: 2.6594 - val_y_pred: 1.2118e-04\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6733 - val_loss: 2.6572 - val_y_pred: 2.1726e-04\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6712 - val_loss: 2.6548 - val_y_pred: 2.4862e-04\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6689 - val_loss: 2.6526 - val_y_pred: 3.9158e-04\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6667 - val_loss: 2.6504 - val_y_pred: 4.8022e-04\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6646 - val_loss: 2.6481 - val_y_pred: 5.0494e-04\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6623 - val_loss: 2.6460 - val_y_pred: 4.7843e-04\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6602 - val_loss: 2.6436 - val_y_pred: 4.7698e-04\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6579 - val_loss: 2.6413 - val_y_pred: 5.0587e-04\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6557 - val_loss: 2.6390 - val_y_pred: 4.8300e-04\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6534 - val_loss: 2.6366 - val_y_pred: 4.2779e-04\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6511 - val_loss: 2.6343 - val_y_pred: 4.8100e-04\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6489 - val_loss: 2.6320 - val_y_pred: 3.6687e-04\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6467 - val_loss: 2.6297 - val_y_pred: 2.9782e-04\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6444 - val_loss: 2.6272 - val_y_pred: 2.0034e-04\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6421 - val_loss: 2.6250 - val_y_pred: 1.5737e-04\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6398 - val_loss: 2.6227 - val_y_pred: 1.9856e-04\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6375 - val_loss: 2.6202 - val_y_pred: 6.2824e-05\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6352 - val_loss: 2.6178 - val_y_pred: 6.1208e-05\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6328 - val_loss: 2.6155 - val_y_pred: 9.6313e-05\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6305 - val_loss: 2.6132 - val_y_pred: 6.2145e-05\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6282 - val_loss: 2.6107 - val_y_pred: -1.1736e-05\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6260 - val_loss: 2.6084 - val_y_pred: 8.6847e-06\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6236 - val_loss: 2.6060 - val_y_pred: -2.9652e-05\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6213 - val_loss: 2.6034 - val_y_pred: -1.3589e-04\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6188 - val_loss: 2.6009 - val_y_pred: -1.6348e-04\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6164 - val_loss: 2.5986 - val_y_pred: -1.5840e-04\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6140 - val_loss: 2.5961 - val_y_pred: -1.4118e-04\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6116 - val_loss: 2.5936 - val_y_pred: -1.3569e-04\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6092 - val_loss: 2.5910 - val_y_pred: -7.9014e-05\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6067 - val_loss: 2.5885 - val_y_pred: -1.4184e-04\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6042 - val_loss: 2.5861 - val_y_pred: -2.9737e-05\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6019 - val_loss: 2.5835 - val_y_pred: -4.3757e-05\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5993 - val_loss: 2.5810 - val_y_pred: -6.3923e-05\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5968 - val_loss: 2.5785 - val_y_pred: 8.2528e-05\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5944 - val_loss: 2.5758 - val_y_pred: -7.0545e-05\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5919 - val_loss: 2.5732 - val_y_pred: 2.6587e-05\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5893 - val_loss: 2.5706 - val_y_pred: 1.6011e-07\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5867 - val_loss: 2.5681 - val_y_pred: -3.2825e-05\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5843 - val_loss: 2.5655 - val_y_pred: 7.9707e-06\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5818 - val_loss: 2.5628 - val_y_pred: 1.4006e-05\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5791 - val_loss: 2.5602 - val_y_pred: -1.9857e-06\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5766 - val_loss: 2.5576 - val_y_pred: -3.6265e-05\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5739 - val_loss: 2.5549 - val_y_pred: -1.0702e-05\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5714 - val_loss: 2.5523 - val_y_pred: -1.7015e-05\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5689 - val_loss: 2.5495 - val_y_pred: -1.7788e-06\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5661 - val_loss: 2.5468 - val_y_pred: 2.4294e-05\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5635 - val_loss: 2.5442 - val_y_pred: 4.6468e-05\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5608 - val_loss: 2.5414 - val_y_pred: -4.3123e-05\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5581 - val_loss: 2.5387 - val_y_pred: -3.4754e-05\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5555 - val_loss: 2.5360 - val_y_pred: -6.2190e-05\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5527 - val_loss: 2.5332 - val_y_pred: -1.6355e-05\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5501 - val_loss: 2.5304 - val_y_pred: -2.1518e-05\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5473 - val_loss: 2.5276 - val_y_pred: 7.2148e-05\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5446 - val_loss: 2.5249 - val_y_pred: 8.1395e-05\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5419 - val_loss: 2.5218 - val_y_pred: 6.6100e-05\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5390 - val_loss: 2.5190 - val_y_pred: -2.4514e-05\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5362 - val_loss: 2.5162 - val_y_pred: 3.4342e-05\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5333 - val_loss: 2.5134 - val_y_pred: -2.9206e-05\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5306 - val_loss: 2.5104 - val_y_pred: -7.5710e-06\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5277 - val_loss: 2.5076 - val_y_pred: -3.4255e-05\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5248 - val_loss: 2.5047 - val_y_pred: -2.7103e-06\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5220 - val_loss: 2.5016 - val_y_pred: 4.6889e-06\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5190 - val_loss: 2.4988 - val_y_pred: -5.7039e-05\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5161 - val_loss: 2.4957 - val_y_pred: -1.4286e-05\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5132 - val_loss: 2.4927 - val_y_pred: -2.1434e-05\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5103 - val_loss: 2.4897 - val_y_pred: -8.1257e-05\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5073 - val_loss: 2.4867 - val_y_pred: -9.7357e-05\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5042 - val_loss: 2.4838 - val_y_pred: -4.6040e-05\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5013 - val_loss: 2.4808 - val_y_pred: -7.8531e-05\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4984 - val_loss: 2.4778 - val_y_pred: -6.3585e-05\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4954 - val_loss: 2.4745 - val_y_pred: -1.0107e-04\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4922 - val_loss: 2.4714 - val_y_pred: -6.5659e-05\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4891 - val_loss: 2.4683 - val_y_pred: -2.5961e-05\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4861 - val_loss: 2.4653 - val_y_pred: -8.5433e-06\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4829 - val_loss: 2.4620 - val_y_pred: -4.0531e-05\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4798 - val_loss: 2.4590 - val_y_pred: -3.8396e-05\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4768 - val_loss: 2.4558 - val_y_pred: -1.5286e-04\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4736 - val_loss: 2.4526 - val_y_pred: -1.6546e-04\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4704 - val_loss: 2.4493 - val_y_pred: -1.0451e-04\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.4672 - val_loss: 2.4460 - val_y_pred: -1.1981e-04\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.4639 - val_loss: 2.4427 - val_y_pred: -1.1911e-04\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.4606 - val_loss: 2.4395 - val_y_pred: -1.1308e-04\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4574 - val_loss: 2.4362 - val_y_pred: -1.2644e-04\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.4541 - val_loss: 2.4329 - val_y_pred: -1.4129e-04\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.4508 - val_loss: 2.4295 - val_y_pred: -1.0072e-04\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4473 - val_loss: 2.4261 - val_y_pred: -9.7012e-05\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.4440 - val_loss: 2.4228 - val_y_pred: -7.1024e-05\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4407 - val_loss: 2.4193 - val_y_pred: -7.6392e-05\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4373 - val_loss: 2.4158 - val_y_pred: -1.3872e-04\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.4338 - val_loss: 2.4123 - val_y_pred: -4.7040e-05\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4303 - val_loss: 2.4088 - val_y_pred: -1.2520e-04\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4268 - val_loss: 2.4053 - val_y_pred: -1.2682e-04\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4234 - val_loss: 2.4017 - val_y_pred: -1.3760e-04\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4198 - val_loss: 2.3983 - val_y_pred: -9.6283e-05\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4163 - val_loss: 2.3947 - val_y_pred: -1.1976e-04\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4127 - val_loss: 2.3910 - val_y_pred: -1.8393e-04\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4090 - val_loss: 2.3873 - val_y_pred: -9.9480e-05\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4054 - val_loss: 2.3837 - val_y_pred: -1.2426e-04\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.4017 - val_loss: 2.3801 - val_y_pred: -2.0642e-04\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3981 - val_loss: 2.3764 - val_y_pred: -2.2771e-04\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3943 - val_loss: 2.3726 - val_y_pred: -1.8905e-04\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3905 - val_loss: 2.3690 - val_y_pred: -1.9553e-04\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3868 - val_loss: 2.3653 - val_y_pred: -1.7954e-04\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3831 - val_loss: 2.3612 - val_y_pred: -1.2745e-04\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.3792 - val_loss: 2.3574 - val_y_pred: -1.8247e-04\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3754 - val_loss: 2.3537 - val_y_pred: -1.7056e-04\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3716 - val_loss: 2.3499 - val_y_pred: -5.1881e-05\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3676 - val_loss: 2.3460 - val_y_pred: -1.2618e-04\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3639 - val_loss: 2.3420 - val_y_pred: -1.0337e-04\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.3598 - val_loss: 2.3382 - val_y_pred: -1.3703e-04\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3559 - val_loss: 2.3342 - val_y_pred: -1.2884e-04\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.3520 - val_loss: 2.3301 - val_y_pred: -1.9247e-04\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.3478 - val_loss: 2.3260 - val_y_pred: -2.0807e-04\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3438 - val_loss: 2.3220 - val_y_pred: -2.0943e-04\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3398 - val_loss: 2.3179 - val_y_pred: -2.0259e-04\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3356 - val_loss: 2.3138 - val_y_pred: -1.8188e-04\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3315 - val_loss: 2.3098 - val_y_pred: -2.3015e-04\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3274 - val_loss: 2.3058 - val_y_pred: -2.0109e-04\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3233 - val_loss: 2.3015 - val_y_pred: -2.1706e-04\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3191 - val_loss: 2.2973 - val_y_pred: -2.6071e-04\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.3148 - val_loss: 2.2930 - val_y_pred: -2.0911e-04\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3105 - val_loss: 2.2888 - val_y_pred: -2.1015e-04\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3063 - val_loss: 2.2846 - val_y_pred: -1.8003e-04\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3020 - val_loss: 2.2805 - val_y_pred: -1.3131e-04\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2977 - val_loss: 2.2763 - val_y_pred: -1.5567e-04\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2934 - val_loss: 2.2717 - val_y_pred: -2.8706e-04\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2890 - val_loss: 2.2674 - val_y_pred: -2.0253e-04\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2846 - val_loss: 2.2631 - val_y_pred: -1.9654e-04\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2802 - val_loss: 2.2587 - val_y_pred: -1.8334e-04\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2758 - val_loss: 2.2544 - val_y_pred: -2.1792e-04\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2714 - val_loss: 2.2500 - val_y_pred: -2.8282e-04\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2669 - val_loss: 2.2456 - val_y_pred: -2.3087e-04\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2624 - val_loss: 2.2411 - val_y_pred: -2.5161e-04\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2579 - val_loss: 2.2366 - val_y_pred: -2.1647e-04\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2533 - val_loss: 2.2320 - val_y_pred: -3.5423e-04\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2487 - val_loss: 2.2276 - val_y_pred: -3.4764e-04\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2442 - val_loss: 2.2230 - val_y_pred: -3.1696e-04\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2395 - val_loss: 2.2185 - val_y_pred: -3.4819e-04\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2349 - val_loss: 2.2139 - val_y_pred: -2.9219e-04\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2303 - val_loss: 2.2092 - val_y_pred: -1.9801e-04\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2256 - val_loss: 2.2046 - val_y_pred: -1.7784e-04\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2208 - val_loss: 2.1998 - val_y_pred: -2.2327e-04\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.2161 - val_loss: 2.1952 - val_y_pred: -2.1370e-04\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2113 - val_loss: 2.1906 - val_y_pred: -1.9440e-04\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2066 - val_loss: 2.1858 - val_y_pred: -1.8321e-04\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2017 - val_loss: 2.1810 - val_y_pred: -2.0323e-04\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1969 - val_loss: 2.1760 - val_y_pred: -1.9384e-04\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1919 - val_loss: 2.1713 - val_y_pred: -3.0681e-04\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.1869 - val_loss: 2.1666 - val_y_pred: -1.8533e-04\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1821 - val_loss: 2.1616 - val_y_pred: -2.2222e-04\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.1770 - val_loss: 2.1566 - val_y_pred: -2.0634e-04\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.1719 - val_loss: 2.1516 - val_y_pred: -3.1518e-04\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.1668 - val_loss: 2.1466 - val_y_pred: -2.8404e-04\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1617 - val_loss: 2.1416 - val_y_pred: -2.1492e-04\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1566 - val_loss: 2.1367 - val_y_pred: -2.6566e-04\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1515 - val_loss: 2.1315 - val_y_pred: -1.9650e-04\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.1462 - val_loss: 2.1263 - val_y_pred: -2.4288e-04\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.1409 - val_loss: 2.1211 - val_y_pred: -2.0549e-04\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.1357 - val_loss: 2.1158 - val_y_pred: -1.4746e-04\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1303 - val_loss: 2.1103 - val_y_pred: -1.8776e-04\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1249 - val_loss: 2.1049 - val_y_pred: -3.3391e-04\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1195 - val_loss: 2.0995 - val_y_pred: -3.3215e-04\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.1140 - val_loss: 2.0939 - val_y_pred: -2.4361e-04\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1085 - val_loss: 2.0885 - val_y_pred: -2.3051e-04\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.1030 - val_loss: 2.0827 - val_y_pred: -3.1577e-04\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0974 - val_loss: 2.0772 - val_y_pred: -3.0280e-04\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0918 - val_loss: 2.0716 - val_y_pred: -3.5060e-04\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0862 - val_loss: 2.0660 - val_y_pred: -3.2661e-04\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0807 - val_loss: 2.0604 - val_y_pred: -4.7187e-04\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0749 - val_loss: 2.0546 - val_y_pred: -3.8932e-04\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0691 - val_loss: 2.0488 - val_y_pred: -3.7108e-04\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0633 - val_loss: 2.0430 - val_y_pred: -4.4661e-04\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0574 - val_loss: 2.0372 - val_y_pred: -3.5135e-04\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0515 - val_loss: 2.0311 - val_y_pred: -3.5746e-04\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0454 - val_loss: 2.0253 - val_y_pred: -3.6670e-04\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0394 - val_loss: 2.0191 - val_y_pred: -3.8423e-04\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0331 - val_loss: 2.0129 - val_y_pred: -4.0667e-04\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0269 - val_loss: 2.0067 - val_y_pred: -3.9770e-04\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0206 - val_loss: 2.0004 - val_y_pred: -4.0746e-04\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.0144 - val_loss: 1.9942 - val_y_pred: -4.0295e-04\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0081 - val_loss: 1.9877 - val_y_pred: -4.4287e-04\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0017 - val_loss: 1.9812 - val_y_pred: -3.9373e-04\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.9953 - val_loss: 1.9747 - val_y_pred: -4.1496e-04\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.9888 - val_loss: 1.9679 - val_y_pred: -4.2937e-04\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.9822 - val_loss: 1.9613 - val_y_pred: -3.5733e-04\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9755 - val_loss: 1.9550 - val_y_pred: -5.4530e-04\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9691 - val_loss: 1.9483 - val_y_pred: -4.9424e-04\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9626 - val_loss: 1.9416 - val_y_pred: -5.8317e-04\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9557 - val_loss: 1.9346 - val_y_pred: -4.6017e-04\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9488 - val_loss: 1.9279 - val_y_pred: -5.1730e-04\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9421 - val_loss: 1.9210 - val_y_pred: -6.0203e-04\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9352 - val_loss: 1.9139 - val_y_pred: -5.3222e-04\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.9281 - val_loss: 1.9072 - val_y_pred: -4.4682e-04\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9212 - val_loss: 1.9002 - val_y_pred: -4.2589e-04\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9143 - val_loss: 1.8933 - val_y_pred: -5.5733e-04\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9071 - val_loss: 1.8863 - val_y_pred: -4.4812e-04\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9001 - val_loss: 1.8790 - val_y_pred: -4.3377e-04\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8927 - val_loss: 1.8719 - val_y_pred: -3.1991e-04\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.8854 - val_loss: 1.8647 - val_y_pred: -4.1604e-04\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8782 - val_loss: 1.8573 - val_y_pred: -4.5722e-04\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.8706 - val_loss: 1.8500 - val_y_pred: -3.1401e-04\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.8632 - val_loss: 1.8425 - val_y_pred: -3.3764e-04\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.8554 - val_loss: 1.8349 - val_y_pred: -3.0453e-04\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8477 - val_loss: 1.8273 - val_y_pred: -3.8111e-04\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8399 - val_loss: 1.8197 - val_y_pred: -1.0962e-04\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8320 - val_loss: 1.8118 - val_y_pred: -1.4638e-04\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.8238 - val_loss: 1.8042 - val_y_pred: 3.6354e-05\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8160 - val_loss: 1.7965 - val_y_pred: -9.3507e-05\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.8079 - val_loss: 1.7887 - val_y_pred: -1.1279e-04\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8000 - val_loss: 1.7808 - val_y_pred: -5.4989e-05\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7918 - val_loss: 1.7728 - val_y_pred: -1.1932e-04\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.7839 - val_loss: 1.7648 - val_y_pred: -3.1701e-06\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7758 - val_loss: 1.7566 - val_y_pred: -1.2724e-04\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7676 - val_loss: 1.7482 - val_y_pred: -6.4672e-05\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7594 - val_loss: 1.7402 - val_y_pred: 3.5819e-05\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7512 - val_loss: 1.7320 - val_y_pred: -1.5447e-04\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = get_data(config)\n",
    "x, y = train_data\n",
    "opt = tf.keras.optimizers.__dict__[config.optimizer](lr=config.lr)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "history = model.fit(x, y, batch_size=101,epochs=config.epochs, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = history.history['loss']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fff3a2c054a5d83053cb95d2ca3b099f5b8f713534c8bf128490ee39121907f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
